{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Chat-Scenes Text generation and comparing it to PointLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* 1.1 We have the generation function  in Chat-Scene\n",
    "\n",
    "outputs = self.llama_model.generate(\n",
    "    inputs_embeds=wrapped_embed,\n",
    "    max_new_tokens=self.max_txt_len,\n",
    "    # stopping_criteria=stopping_criteria,\n",
    "    num_beams=5,\n",
    "    # do_sample=True,\n",
    "    min_length=1,\n",
    "    # top_p=0.9,\n",
    "    repetition_penalty=3.0,\n",
    "    length_penalty=1,\n",
    "    temperature=1.0,\n",
    "    customized_mask=attention_mask\n",
    ")\n",
    "output_token = outputs[0]\n",
    "output_text = self.llama_tokenizer.decode(output_token)\n",
    "output_text = output_text.split(self.end_sym)[0]\n",
    "output_text = output_text.replace('  ', ' ').replace(' .', '.').strip()\n",
    "output_text = recover_caption(output_text, assigned_ids[i].tolist())\n",
    "output_texts.append(output_text)\n",
    "print(\"output\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* 1.2 And similarly in PointLLM\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    point_clouds=point_clouds,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    max_length=2048,\n",
    "    top_p=0.95,\n",
    "    stopping_criteria=[stopping_criteria])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each component in Chat-Scenes generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input_embeds = wrapped_embed = torch.cat([p_0_embed, object_list_embed, p_1_embed, prompt_embed], dim=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#p_0 and p_1 embedding:\n",
    "#Embeddings giving the LLM model an idea about the situation and its role\n",
    "p_0_embed = self.system + \" \" + self.instruction \n",
    "\n",
    "self.system: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "self. The conversation centers around an indoor scene. Object information:\n",
    "\n",
    "p_1_embed = self.role[0]  #  => (\"USER\", \"ASSISTANT\")\n",
    "\n",
    "# prompt_emb:\n",
    "# Replace template with the predicted object IDS, then tokenize the text_embeddings for the LLM\n",
    "tmp_prompt = f\" {custom_prompt[i]} {self.role[1]}: \"\n",
    "tmp_prompt = update_caption(tmp_prompt, assigned_ids[i])  # replace placeholders\n",
    "prompt_embed = self.get_text_emb(tmp_prompt, device=device) # embedd into LDM space\n",
    "\n",
    "# object_list_embed \n",
    "# Generate embeddings of the objects by comining object, image and scence embeddings, and object ids and asigned ids.\n",
    "\n",
    "\n",
    "object_list_embed = self.get_object_list_embed(\n",
    "    proj_object_embed[i],                                       # linear layer on x, _ =  encode_object_feat(scene_feat, scene_img_feat, scene_locs)\n",
    "    proj_object_img_embed[i] if self.add_img_token else None,  # linear layer on of _, y =  encode_object_feat()\n",
    "    proj_scene_embed[i] if self.add_scene_token else None,      # also combination of multiple projection of encode_object_feat()\n",
    "    scene_mask[i],    # Is given\n",
    "    obj_ids[i],       # Is given\n",
    "    assigned_ids[i]   # Is given\n",
    ")\n",
    "\n",
    "    # object_list proj_scene_emb:\n",
    "    # A series of transformation. We need to pass object_emb\n",
    "    obj_embed = self.scene_init_proj(object_embed)\n",
    "    mins, maxs = self.get_min_max_coord(scene_locs[:, :, :3], scene_mask)\n",
    "    pos_embed = self.pos_embedding(scene_locs[:, :, :3], input_range=[mins, maxs])\n",
    "    pos_embed = self.pos_proj(pos_embed)\n",
    "    scene_embed = obj_embed + pos_embed\n",
    "    scene_embed = self.relation_module(scene_embed, src_key_padding_mask=~scene_mask)\n",
    "    proj_scene_embed = self.scene_proj(scene_embed)\n",
    "\n",
    "    # proj_object_embed & roj_object_img_embed\n",
    "    # two easy linear layers\n",
    "    object_embed, object_img_embed = self.encode_object_feat(scene_feat, scene_img_feat, scene_locs)\n",
    "    device = object_embed.device\n",
    "    batch_size, obj_num = object_embed.shape[:2]\n",
    "    proj_object_embed = self.object_proj(object_embed)\n",
    "    proj_object_img_embed = self.object_img_proj(object_img_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem, scence_feat, scene_img_scene_locs and thus object embeddings are all pre calculated with no function in the project to calculate them on the fly f.e:\n",
    "{'objects': ['wall', 'wall', 'door', 'table', 'board', 'board', 'window', 'backpack', 'trash can', 'chair', 'window', 'object', 'whiteboard', 'window', 'whiteboard', 'floor', 'shelf'], 'locs': tensor([[ 0.4843,  3.0964,  1.1317,  2.9031,  0.4073,  1.5365],\n",
    "        [ 1.9221,  0.2821,  0.9044,  0.3380,  5.9967,  1.7973],\n",
    "        [ 1.9491,  2.5492,  1.0719,  0.1746,  1.1071,  1.3984],\n",
    "        [ 1.0610, -0.3603,  0.4325,  1.6986,  0.8621,  0.8731],\n",
    "        [ 1.8779,  1.4446,  0.6964,  0.1810,  1.3372,  1.0040],\n",
    "        [ 0.1698,  2.9925,  1.0072,  0.7949,  0.4539,  1.3176],\n",
    "        [ 1.9896,  0.4923,  0.8999,  0.3169,  0.8710,  1.0059],\n",
    "        [ 1.1553, -0.2838,  0.9101,  0.5141,  0.4913,  0.2214],\n",
    "        [ 0.2111, -0.3626,  0.2123,  0.2265,  0.3294,  0.4584],\n",
    "        [ 1.2761, -0.5555,  0.2638,  0.6453,  0.4383,  0.4702],\n",
    "        [ 1.9756, -1.1988,  0.8657,  0.2644,  0.6797,  1.1207],\n",
    "        [ 1.8489, -1.3361,  0.0752,  0.1166,  0.1187,  0.0509],\n",
    "        [ 1.3893, -1.7899,  0.6864,  0.8835,  0.3986,  1.3962],\n",
    "        [ 2.0321, -2.6150,  0.3549,  0.2220,  0.4936,  0.1530],\n",
    "        [ 1.2083, -2.6693,  0.5254,  1.7060,  0.3711,  1.1552],\n",
    "        [ 0.5395, -0.4110,  0.2262,  2.9660,  7.4137,  0.6083],\n",
    "        [-1.0287, -0.3186,  0.9265,  0.6522,  6.6778,  1.9152]])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
